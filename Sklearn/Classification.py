import pandas as pd
import numpy
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
import time
from func_timeout import func_timeout
from Wrapping import ffun

def cv_column_filter(df):

    df=df.loc[:, [c for c in df.columns
               if (c[:6] == "param_")]]
    return df


def classification(file, dataset, classifiers_names, classifier_functions, randomized_search_functions, columns):

    print(file, flush=True)
    class_col = dataset.iloc[:,-1]
    dataset = pd.get_dummies(dataset.iloc[:,:-1])
    dataset["class"] = class_col

    # Split the dataset into training and test data
    X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[:,0:-1], dataset.iloc[:,-1],
                                                        train_size=0.8, test_size=0.2, random_state=0)

    i = 0

    for name in classifiers_names:
        print(name, flush=True)
        # Select the classifier to apply
        classifier = classifier_functions[i]

        # Select the RandomizedSearch function to apply  accordaning to classifier
        random_search = randomized_search_functions[i]
        log_size = random_search.get_params()["n_iter"]
        logs = pd.DataFrame(index=range(0, log_size), columns=columns)
        i = i + 1

        # Fit training values to RandomizedSearchCV
        fun = ffun(X_train,y_train,random_search)

        # Apply timeout for large datasets
        try:
            func_timeout(12000, fun.fun, args=() ) # maximum 5 minutes per 1 iteration and we have max 40 iterations --> 40 * 5 * 60 = 12000
        except (BaseException, Exception, numpy.linalg.LinAlgError) as error:
            print(error, flush=True)
            logs.loc[:,"Dataset"] = file
            logs.loc[:,"Classifier"] = "sklearn." + name
            logs.to_csv("ClassifierLogs.csv", header=False, mode='a', columns=columns, index=False)
            continue


        # Store the results in cv_results
        cv_results = pd.DataFrame(random_search.cv_results_)

        # Filter only columns that show parameter values
        cv_results = cv_column_filter(cv_results)

        #############################################################
        ### Run the classifier for every combination of parameters


        for j in range(cv_results.shape[0]):
            logs.at[j, "Dataset"] = file
            logs.at[j, "Classifier"] = "sklearn."+name

            # Store parameters generated by RandomizedSearchCV in a dictionary.
            parameter_values = {}

            for c in cv_results.columns:
                column = c[6:]

                parameter_values[column] = cv_results.iloc[j][c]

                # Save it to logs dataframe as well
                logs.at[j, column] = cv_results.iloc[j][c]

            # Update the classifier parameters via set_params() method
            function = classifier.set_params(**parameter_values)

            # Fit the function and measure the time in fractional seconds
            try:
                start = time.perf_counter()
                function.fit(X_train, y_train)
                end = time.perf_counter()
            except (BaseException, numpy.linalg.LinAlgError) as error:
                print(error,flush=True)
                continue

            train_time = end - start
            logs.at[j, "Train time"] = train_time

            # Predict labels for train data
            pred_train = function.predict(X_train)

            # Predict labels for test data and measure time in fractional seconds
            start = time.perf_counter()
            pred_test = function.predict(X_test)
            end = time.perf_counter()
            test_time = end - start
            logs.at[j, "Test time"] = test_time

            # Calculate train and test data accuracy and store the results in logs dataframe
            train_accuracy = accuracy_score(y_train, pred_train)
            test_accuracy = accuracy_score(y_test, pred_test)
            logs.at[j, "Train accuracy"] = train_accuracy
            logs.at[j, "Test accuracy"] = test_accuracy

            # Calculate train and test data recall and store the results in logs dataframe.
            # For multiclass classification problems specify average method.
            # Calculate metrics globally by considering each element of the label indicator matrix as a label.
            train_recall = recall_score(y_train, pred_train, average="micro")
            test_recall = recall_score(y_test, pred_test, average="micro")
            logs.at[j, "Train recall"] = train_recall
            logs.at[j, "Test recall"] = test_recall

            # Calculate train and test data precision and store the results in logs dataframe.
            # For multiclass classification problems specify average method.
            # Calculate metrics globally by considering each element of the label indicator matrix as a label.
            train_precision = precision_score(y_train, pred_train, average="micro")
            test_precision = precision_score(y_test, pred_test, average="micro")
            logs.at[j, "Train precision"] = train_precision
            logs.at[j, "Test precision"] = test_precision

            # Calculate train and test data F1 score and store the results in logs dataframe.
            # For multiclass classification problems specify average method.
            # Calculate metrics globally by considering each element of the label indicator matrix as a label.
            train_f1_score = f1_score(y_train, pred_train, average="micro")
            test_f1_score = f1_score(y_test, pred_test, average="micro")
            logs.at[j, "Train f1_score"] = train_f1_score
            logs.at[j, "Test f1_score"] = test_f1_score

        # Write logs to file
        logs.to_csv("ClassifierLogs.csv", header=False, mode='a', columns=columns, index=False)

